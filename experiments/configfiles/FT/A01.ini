[EXPERIMENTS]
# Type of trainig, FT for this paper (Fine-Tuning)
mode = FT
# Directory to save the results of the experiment
dir = outputs

[PROCESSING]
# Ratio of data used for training
# The remaining is kept as a validation set
# Note: we use all for ft in low data regime
# for generation
split = 1.0
# Minimum SMILES length 
min_len = 20
# Maximum SMILES length 
max_len = 90
# If you want to randomized the SMILES encoding,
# i.e. not using the same version for all (e.g. canonical)
randomized_smi = False

[AUGMENTATION]
# SMILES augmentation
# If 0, no augmentation
fold = 0

[MODEL]
# Number of training epochs. 100 in the paper
epochs = 4
# Learning rate
lr = 0.0001
# List with number of neurons per layer
# e.g. if two element in the list, two layers
neurons = [1024,256]
# Dropout applied to the layers above
# Need the same number of element as the list for neurons
dropouts = [0.40, 0.40]
# If True, the layer(s) defined in neurons are trainable
trainables = [True, True]
# Number of epoch before reducing the learning rate
# if the loss does not improve
patience_lr = 3
# Every period epoch a model will be saved
period = 2
# Factor to decrease the learning rate if 
# it does not improve for patience_lr epochs
factor = 0.5
# Minimum learning rate allowed when the learning
# rate is decreasing
min_lr = 0.00005
# Batch size
batch_size = 5
n_workers = 4

[SAMPLING]
# Top k sampling paramter; not used in this paper
top_k = 0
# Parameter for nucleus sampling; not used in this paper
top_p = 0.0
# Number of SMILES to sample by epoch; 1000 in the paper
n_sample = 100
# Temperature parameter
temp = 1.00

[FINETUNING]
# Path to the pretrained CLM to be used
# for fine-tuning
LM = ../pretrained_models/CLM.h5

[DATA]
# Directory where fine-tuning files are, 
# in folder having the name of the
# protein target. Note that the protein target name
# are explicitely given in the bash files used to 
# run the experiments
dir = ../data/ft_files/
# Directory where the pretraining data are.
# This data is used to check the novelty of the
# generated SMILES strings, as the pretrained CLM
# is already provided
pretraining_dir = ../data/pretraining_data/

